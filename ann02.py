# -*- coding: utf-8 -*-
"""ann02.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gWa1RtkhO2j3b9BKP4xVCJDVZbtFg7Y4
"""

import numpy as np
import matplotlib.pyplot as plt

# Activation functions
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def relu(x):
    return np.maximum(0, x)

def tanh(x):
    return np.tanh(x)

def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum()

# Input range
x = np.linspace(-5, 5, 100)

# Compute activation function values
y_sigmoid = sigmoid(x)
y_relu = relu(x)
y_tanh = tanh(x)
y_softmax = softmax(x)

# Plotting
plt.figure(figsize=(5, 3))

# Sigmoid
plt.plot(x, y_sigmoid, 'b-', label='Sigmoid')
plt.title('Sigmoid Activation')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.legend()

plt.tight_layout()
plt.show()

# ReLU
plt.figure(figsize=(5, 3))
plt.plot(x, y_relu, 'r-', label='ReLU')
plt.title('ReLU Activation')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.legend()

plt.tight_layout()
plt.show()

# Tanh
plt.figure(figsize=(5, 3))
plt.plot(x, y_tanh, 'g-', label='Tanh')
plt.title('Tanh Activation')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.legend()

plt.tight_layout()
plt.show()

# Softmax
plt.figure(figsize=(5, 3))
plt.plot(x, y_softmax, 'm-', label='Softmax')
plt.title('Softmax Activation')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.legend()

plt.tight_layout()
plt.show()

